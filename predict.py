import pandas as pd
import numpy as np
import joblib
import os
import shap
import sys

# --- CONFIGURATION ---
MODEL_DIR = "models"
MODEL_PATH = os.path.join(MODEL_DIR, "attrition_model.pkl")
COLUMNS_PATH = os.path.join(MODEL_DIR, "columns.pkl")
# On a besoin d'un dataset de r√©f√©rence pour SHAP (background distribution)
# Id√©alement on prendrait X_train, ici on va charger processed_data pour en extraire un √©chantillon
DATA_FILE = "processed_data.csv"

def get_user_input(model_columns):
    """Demande √† l'utilisateur de saisir les valeurs pour les features principales."""
    print("\n--- Saisie des informations de l'employ√© ---")
    print("(Appuyez sur Entr√©e pour utiliser la valeur par d√©faut/m√©diane)")
    
    input_data = {}
    
    # Dictionnaire des valeurs par d√©faut
    defaults = {
        'Age': 35,
        'MonthlyIncome': 65000,
        'TotalWorkingYears': 10,
        'YearsAtCompany': 5,
        'YearsWithCurrManager': 3,
        'DistanceFromHome': 10,
        'MeanWorkingHours': 7.5,
        'WorkDays': 250,
        'OverTimeFrequency': 0.1, # Ceci est une valeur num√©rique d√©riv√©e, ne pas confondre avec la cat√©gorielle 'OverTime'
        'EnvironmentSatisfaction': 3,
        'JobSatisfaction': 3,
        'WorkLifeBalance': 3,
        # Valeurs par d√©faut pour les cat√©gories
        'BusinessTravel': 'Travel_Rarely',
        'Department': 'Research & Development',
        'EducationField': 'Life Sciences',
        'Gender': 'Male',
        'JobRole': 'Sales Executive',
        'MaritalStatus': 'Married',
        'OverTime': 'No' # C'est la cat√©gorielle, pas la fr√©quence
    }

    # Liste des features num√©riques cl√©s √† demander
    key_numerical_features = [
        'Age', 'MonthlyIncome', 'TotalWorkingYears', 'YearsAtCompany', 
        'YearsWithCurrManager', 'DistanceFromHome', 'MeanWorkingHours', 
        'WorkDays', 'OverTimeFrequency', # OverTimeFrequency est num√©rique ici
        'EnvironmentSatisfaction', 'JobSatisfaction', 'WorkLifeBalance'
    ]

    # Liste des features cat√©gorielles cl√©s √† demander avec leurs options
    key_categorical_features = {
        'BusinessTravel': ['Non-Travel', 'Travel_Rarely', 'Travel_Frequently'],
        'Department': ['Research & Development', 'Sales', 'Human Resources'],
        'EducationField': ['Life Sciences', 'Medical', 'Marketing', 'Technical Degree', 'Other', 'Human Resources'],
        'Gender': ['Female', 'Male'],
        'JobRole': ['Sales Executive', 'Research Scientist', 'Laboratory Technician', 'Manufacturing Director', 'Healthcare Representative', 'Manager', 'Sales Representative', 'Research Director', 'Human Resources'],
        'MaritalStatus': ['Single', 'Married', 'Divorced'],
        'OverTime': ['No', 'Yes']
    }
    
    # Validation des plages r√©alistes pour les num√©riques
    validation_rules = {
        'Age': (18, 70),
        'MonthlyIncome': (1000, 500000),
        'TotalWorkingYears': (0, 50),
        'YearsAtCompany': (0, 50),
        'YearsWithCurrManager': (0, 50),
        'DistanceFromHome': (0, 100),
        'MeanWorkingHours': (0, 24),
        'WorkDays': (0, 300),
        'OverTimeFrequency': (0.0, 1.0),
        'EnvironmentSatisfaction': (1, 4),
        'JobSatisfaction': (1, 4),
        'WorkLifeBalance': (1, 4)
    }

    # Saisie des num√©riques
    for feature in key_numerical_features:
        default_val = defaults.get(feature, 0)
        min_val, max_val = validation_rules.get(feature, (0, float('inf')))
        
        while True:
            val = input(f"{feature} (d√©faut: {default_val}) [{min_val}-{max_val}]: ")
            if val == "":
                input_data[feature] = default_val
                break
            try:
                float_val = float(val)
                if min_val <= float_val <= max_val:
                    input_data[feature] = float_val
                    break
                else:
                    print(f"Erreur : La valeur doit √™tre comprise entre {min_val} et {max_val}.")
            except ValueError:
                print("Veuillez entrer un nombre valide.")
    
    # Saisie des cat√©gorielles
    for feature, options in key_categorical_features.items():
        default_val = defaults.get(feature, options[0])
        print(f"\n--- {feature} (d√©faut: {default_val}) ---")
        for i, option in enumerate(options):
            print(f"{i+1}. {option}")
        
        while True:
            choice = input(f"Votre choix (1-{len(options)}, Entr√©e pour d√©faut): ")
            if choice == "":
                input_data[feature] = default_val
                break
            try:
                choice_idx = int(choice) - 1
                if 0 <= choice_idx < len(options):
                    input_data[feature] = options[choice_idx]
                    break
                else:
                    print(f"Erreur : Veuillez choisir un nombre entre 1 et {len(options)}.")
            except ValueError:
                print("Veuillez entrer un nombre valide.")
                
    return input_data

def prepare_input_dataframe(user_input, model_columns):
    """Cr√©e un DataFrame pr√™t pour le mod√®le avec toutes les colonnes √† 0 sauf celles saisies."""
    # Cr√©er un DF avec une seule ligne remplie de z√©ros
    df = pd.DataFrame(0, index=[0], columns=model_columns)
    
    # Remplir avec les donn√©es utilisateur num√©riques
    for col, val in user_input.items():
        if col in df.columns: # Pour les colonnes num√©riques directes
            df[col] = val
        # Pour les colonnes cat√©gorielles, on cherche la colonne one-hot encod√©e
        else: 
            # Reconstituer le nom de la colonne one-hot encod√©e
            # Exemple: BusinessTravel_Travel_Frequently
            if col == 'Gender' and val == 'Male': # Si Male, on active Gender_Male
                if 'Gender_Male' in df.columns:
                    df['Gender_Male'] = 1
            elif col == 'OverTime' and val == 'Yes': # Si Yes, on active OverTime_Yes
                if 'OverTime_Yes' in df.columns:
                    df['OverTime_Yes'] = 1
            else:
                dummy_col_name = f"{col}_{val}"
                if dummy_col_name in df.columns:
                    df[dummy_col_name] = 1
    
    return df

def list_and_select_model():
    files = [f for f in os.listdir(MODEL_DIR) if f.endswith('.pkl') and f not in ['columns.pkl', 'encoders.pkl']]
    if not files:
        print(f"Aucun mod√®le trouv√© dans {MODEL_DIR}. Veuillez lancer train_model.py.")
        sys.exit(1)
        
    print("\n--- CHOIX DU MOD√àLE ---")
    files.sort()
    for i, f in enumerate(files):
        print(f"{i+1}. {f}")
        
    while True:
        choice = input(f"Votre choix (1-{len(files)}) : ")
        if choice.isdigit() and 1 <= int(choice) <= len(files):
            return os.path.join(MODEL_DIR, files[int(choice)-1])
        print("Choix invalide.")

def predict_and_explain():
    # 1. S√©lection et Chargement
    model_path = list_and_select_model()
    print(f"Chargement du mod√®le : {model_path}...")
    
    try:
        model = joblib.load(model_path)
        model_columns = joblib.load(COLUMNS_PATH)
    except FileNotFoundError:
        print("Erreur : Fichiers manquants (mod√®le ou columns.pkl).")
        sys.exit(1)
        
    # 2. Saisie utilisateur
    user_data = get_user_input(model_columns)
    input_df = prepare_input_dataframe(user_data, model_columns)
    
    # 3. Pr√©diction
    print("\n--- Analyse en cours... ---")
    prediction = model.predict(input_df)[0]
    
    # Gestion proba (certains mod√®les comme SVM n'ont pas predict_proba par d√©faut, mais ici on a LogReg/RF/XGB/NB qui l'ont)
    if hasattr(model, "predict_proba"):
        probability = model.predict_proba(input_df)[0][1]
    else:
        probability = 0.5 # Fallback ou 1.0/0.0 selon la pr√©diction
    
    print(f"\nR√©sultat : {'‚ö†Ô∏è RISQUE DE D√âPART (Oui)' if prediction == 1 else '‚úÖ FID√âLISATION PROBABLE (Non)'}")
    print(f"Probabilit√© de d√©part estim√©e : {probability:.1%}")
    
    # 4. Explication SHAP
    print("\nCalcul des facteurs d'influence (SHAP values)...")
    
    # Gestion Pipeline
    estimator = model
    explainer_input = input_df
    
    if hasattr(model, 'named_steps'):
        # C'est un pipeline
        estimator = model.steps[-1][1]
        preprocessor = model.steps[:-1] # Tout sauf le dernier
        
        # On transforme l'entr√©e pour SHAP (car l'explainer doit voir les donn√©es scal√©es)
        # Attention: Pipeline slice returns a Pipeline, we can fit_transform or just transform if already fitted
        # Le mod√®le global est d√©j√† fitt√©, donc les steps aussi.
        # On applique les transformations s√©quentiellement
        for name, step in preprocessor:
             if hasattr(step, 'transform'):
                explainer_input = step.transform(explainer_input)
    
    # D√©tection du type de mod√®le pour l'explainer
    class_name = estimator.__class__.__name__
    explainer = None
    shap_values_obj = None  # Initialisation pour √©viter UnboundLocalError
    
    # Background dataset n√©cessaire pour Linear et Kernel
    # On charge un petit √©chantillon et on le transforme de la m√™me fa√ßon si pipeline
    try:
        if os.path.exists(DATA_FILE):
             # Chargement partiel pour la baseline
             df_bg = pd.read_csv(DATA_FILE).sample(100, random_state=42)
             # Ici c'est compliqu√© car on a besoin que df_bg ait les m√™mes colonnes que input_df (one-hot encoded)
             # Or DATA_FILE est brut ou processed? DATA_FILE est "processed_data.csv" qui est d√©j√† clean mais pas one-hot pour l'affichage?
             # Ah, DATA_FILE a d√©j√† les features num√©riques/cat√©gorielles.
             # Si processed_data.csv est utilis√© dans train(), il est transform√© via get_dummies.
             # On ne peut pas facilement reproduire le get_dummies ici sans re-impl√©menter la logique de train().
             # Simplification: Background de z√©ros pour SHAP Linear
             background = pd.DataFrame(0, index=np.arange(10), columns=model_columns)
             
             # Si pipeline, on transforme le background
             if hasattr(model, 'named_steps'):
                 for name, step in model.steps[:-1]:
                     if hasattr(step, 'transform'):
                         background = step.transform(background)
        else:
             background = pd.DataFrame(0, index=np.arange(10), columns=model_columns)
             if hasattr(model, 'named_steps'):
                 for name, step in model.steps[:-1]:
                     if hasattr(step, 'transform'):
                         background = step.transform(background)
    except:
        background = pd.DataFrame(0, index=np.arange(10), columns=model_columns)

    if class_name in ['RandomForestClassifier', 'XGBClassifier', 'GradientBoostingClassifier']:
        explainer = shap.TreeExplainer(estimator)
        # TreeExplainer g√®re tout seul ou n√©cessite data selon le mod√®le (sklearn RF a besoin que d'interne souvent, XGB aussi)
        shap_values_obj = explainer(explainer_input) # Nouvelle API
        shap_values = shap_values_obj.values
        
    elif class_name in ['LogisticRegression']:
        # LinearExplainer a besoin d'un masker (background data)
        explainer = shap.LinearExplainer(estimator, background)
        shap_values_obj = explainer(explainer_input)
        shap_values = shap_values_obj.values
        
    else:
        # Fallback (Naive Bayes, etc.) -> KernelExplainer (lent mais g√©n√©rique)
        # Ou on skip si c'est trop compliqu√©
        print(f"Mod√®le {class_name} : Utilisation de KernelExplainer (peut √™tre lent)...")
        try:
             # KernelExplainer a besoin de la fonction de pr√©diction de proba
             # Si Pipeline, attention: estimator.predict_proba attend input scal√©
             # Si on passe estimator.predict_proba, SHAP passera des inputs perturb√©s "scal√©s" (bas√© sur background scal√©)
             predict_fn = estimator.predict_proba
             explainer = shap.KernelExplainer(predict_fn, background)
             shap_values = explainer.shap_values(explainer_input)
             # Kernel retourne souvent une liste pour classification
        except Exception as e:
             print(f"Impossible de calculer SHAP pour ce mod√®le : {e}")
             shap_values = None

    if shap_values is not None:
        # Standardisation des dimensions de shap_values
        vals = None
        
        # Si c'est un objet Explanation (nouvelle API)
        if shap_values_obj is not None and hasattr(shap_values_obj, "shape"): # C'est un objet Explanation ou array
             if len(shap_values.shape) == 2: # (n_samples, n_features) -> cas XGBoost binaire output margin parfois, ou LogReg
                  vals = shap_values[0]
             elif len(shap_values.shape) == 3: # (n_samples, n_features, n_classes)
                  vals = shap_values[0, :, 1] # Classe 1
        
        # Si c'est l'ancienne API (liste de arrays)
        if vals is None and isinstance(shap_values, list):
            if len(shap_values) > 1:
                vals = shap_values[1][0] # Classe 1
            else:
                 vals = shap_values[0][0] # Cas rare
        elif vals is None and isinstance(shap_values, np.ndarray):
             if len(shap_values.shape) == 2:
                  vals = shap_values[0]
        
        # Affichage
        if vals is not None:
            feature_importance = pd.DataFrame(list(zip(model_columns, vals)), columns=['Feature', 'SHAP_Value'])
            feature_importance['Abs_Value'] = feature_importance['SHAP_Value'].abs()
            feature_importance = feature_importance.sort_values(by='Abs_Value', ascending=False).head(7)
            
            print("\n--- POURQUOI CE R√âSULTAT ? (Top 7 Facteurs) ---")
            print("Une valeur positive (+) augmente le risque de d√©part.")
            print("Une valeur n√©gative (-) r√©duit le risque (fid√©lise).")
            print("-" * 60)
            
            for index, row in feature_importance.iterrows():
                feature = row['Feature']
                shap_val = row['SHAP_Value']
                user_val = input_df[feature].values[0]
                
                direction = "Augmente le risque üî¥" if shap_val > 0 else "R√©duit le risque üü¢"
                print(f"{feature:<25} : {user_val:>10.2f}  | Impact: {shap_val:>6.2f} ({direction})")
            print("-" * 60)
        else:
             print("Format SHAP non reconnu, impossible d'afficher les d√©tails.")
    else:
        print("Pas d'explication d√©taill√©e disponible pour ce mod√®le.")

if __name__ == "__main__":
    predict_and_explain()
